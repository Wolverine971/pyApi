{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'title'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ed2333b39cb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mloop_over_hashes_and_remove_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mmainIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'intj'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-ed2333b39cb3>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m#for index in es.indices.get('*'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m#if(re.match(r'^[e|i]', index)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mscroll_over_all_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mloop_over_hashes_and_remove_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mmainIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'intj'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ed2333b39cb3>\u001b[0m in \u001b[0;36mscroll_over_all_docs\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscroll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscroll_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscroll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Process current batch of hits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mpopulate_dict_of_duplicate_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;31m# Update the scroll ID\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0msid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_scroll_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ed2333b39cb3>\u001b[0m in \u001b[0;36mpopulate_dict_of_duplicate_docs\u001b[1;34m(hits)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mcombined_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmykey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys_to_include_in_hash\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mcombined_key\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_source'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmykey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0m_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mhashval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_key\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "# import hashlib\n",
    "# from elasticsearch import Elasticsearch\n",
    "# #https://www.elastic.co/blog/how-to-find-and-remove-duplicate-documents-in-elasticsearch\n",
    "# es = Elasticsearch([\"localhost:9200\"])\n",
    "# dict_of_duplicate_docs = {}\n",
    "# # The following line defines the fields that will be\n",
    "# # used to determine if a document is a duplicate\n",
    "# keys_to_include_in_hash = [\"url\", \"title\"]\n",
    "# # Process documents returned by the current search/scroll\n",
    "# def populate_dict_of_duplicate_docs(hits):\n",
    "#     for item in hits:\n",
    "#         combined_key = \"\"\n",
    "#         for mykey in keys_to_include_in_hash:\n",
    "#             combined_key += str(item['_source'][mykey])\n",
    "#         _id = item[\"_id\"]\n",
    "#         hashval = hashlib.md5(combined_key.encode('utf-8')).digest()\n",
    "#         # If the hashval is new, then we will create a new key\n",
    "#         # in the dict_of_duplicate_docs, which will be\n",
    "#         # assigned a value of an empty array.\n",
    "#         # We then immediately push the _id onto the array.\n",
    "#         # If hashval already exists, then\n",
    "#         # we will just push the new _id onto the existing array\n",
    "#         dict_of_duplicate_docs.setdefault(hashval, []).append(_id)\n",
    "# # Loop over all documents in the index, and populate the\n",
    "# # dict_of_duplicate_docs data structure.\n",
    "# def scroll_over_all_docs():\n",
    "#     data = es.search(index='comment', scroll='1m',  body={\"query\": {\"match_all\": {}}})\n",
    "#     # Get the scroll ID\n",
    "#     sid = data['_scroll_id']\n",
    "#     scroll_size = len(data['hits']['hits'])\n",
    "#     # Before scroll, process current batch of hits\n",
    "#     populate_dict_of_duplicate_docs(data['hits']['hits'])\n",
    "#     while scroll_size > 0:\n",
    "#         data = es.scroll(scroll_id=sid, scroll='2m')\n",
    "#         # Process current batch of hits\n",
    "#         populate_dict_of_duplicate_docs(data['hits']['hits'])\n",
    "#         # Update the scroll ID\n",
    "#         sid = data['_scroll_id']\n",
    "#         # Get the number of results that returned in the last scroll\n",
    "#         scroll_size = len(data['hits']['hits'])\n",
    "# def loop_over_hashes_and_remove_duplicates():\n",
    "#     # Search through the hash of doc values to see if any\n",
    "#     # duplicate hashes have been found\n",
    "#     for hashval, array_of_ids in dict_of_duplicate_docs.items():\n",
    "#       if len(array_of_ids) > 1:\n",
    "#         print(\"********** Duplicate docs hash=%s **********\" % hashval)\n",
    "#         # Get the documents that have mapped to the current hashval\n",
    "#         matching_docs = es.mget(index=mainIndex, doc_type=\"doc\", body={\"ids\": array_of_ids})\n",
    "#         #for doc in matching_docs['docs']:\n",
    "#         for num, doc in enumerate(matching_docs['docs'], start=2):\n",
    "#             # In this example, we just print the duplicate docs.\n",
    "#             # This code could be easily modified to delete duplicates\n",
    "#             # here instead of printing them\n",
    "#             print(\"doc=%s\\n\" % doc)\n",
    "#             #DELETE /<index>/_doc/<_id>\n",
    "#             if num > 2:\n",
    "#                 es.delete(index=mainIndex, id=doc[\"_id\"])\n",
    "# def main():\n",
    "#     #for index in es.indices.get('*'):\n",
    "#         #if(re.match(r'^[e|i]', index)):\n",
    "#             scroll_over_all_docs()\n",
    "#             loop_over_hashes_and_remove_duplicates()\n",
    "# mainIndex = 'intj'\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hashlib\n",
    "# import asyncio\n",
    "\n",
    "# from elasticsearch import Elasticsearch\n",
    "# #https://www.elastic.co/blog/how-to-find-and-remove-duplicate-documents-in-elasticsearch\n",
    "# es = Elasticsearch([\"localhost:9200\"])\n",
    "# dict_of_duplicate_docs = {}\n",
    "# # The following line defines the fields that will be\n",
    "# # used to determine if a document is a duplicate\n",
    "# keys_to_include_in_hash = [\"url\", \"title\"]\n",
    "# # Process documents returned by the current search/scroll\n",
    "# async def populate_dict_of_duplicate_docs(hits):\n",
    "    \n",
    "#     for item in hits:\n",
    "#         combined_key = \"\"\n",
    "#         for mykey in keys_to_include_in_hash:\n",
    "#             combined_key += str(item['_source'][mykey])\n",
    "#         _id = item[\"_id\"]\n",
    "#         hashval = hashlib.md5(combined_key.encode('utf-8')).digest()\n",
    "#         # If the hashval is new, then we will create a new key\n",
    "#         # in the dict_of_duplicate_docs, which will be\n",
    "#         # assigned a value of an empty array.\n",
    "#         # We then immediately push the _id onto the array.\n",
    "#         # If hashval already exists, then\n",
    "#         # we will just push the new _id onto the existing array\n",
    "#         dict_of_duplicate_docs.setdefault(hashval, []).append(_id)\n",
    "#         return dict_of_duplicate_docs\n",
    "# # Loop over all documents in the index, and populate the\n",
    "# # dict_of_duplicate_docs data structure.\n",
    "# def scroll_over_all_docs():\n",
    "#     dict_of_duplicate_docs = {}\n",
    "#     data = es.search(index='istp', scroll='1m',  body={\"query\": {\"match_all\": {}}})\n",
    "#     # Get the scroll ID\n",
    "#     sid = data['_scroll_id']\n",
    "#     scroll_size = len(data['hits']['hits'])\n",
    "#     # Before scroll, process current batch of hits\n",
    "#     populate_dict_of_duplicate_docs(data['hits']['hits'])\n",
    "#     while scroll_size > 0:\n",
    "#         data = es.scroll(scroll_id=sid, scroll='2m')\n",
    "#         # Process current batch of hits\n",
    "#         populate_dict_of_duplicate_docs(data['hits']['hits'])\n",
    "#         # Update the scroll ID\n",
    "#         sid = data['_scroll_id']\n",
    "#         # Get the number of results that returned in the last scroll\n",
    "#         scroll_size = len(data['hits']['hits'])\n",
    "        \n",
    "    \n",
    "        \n",
    "#     #await loop_over_hashes_and_remove_duplicates(index)\n",
    "    \n",
    "# def loop_over_hashes_and_remove_duplicates():\n",
    "#     # Search through the hash of doc values to see if any\n",
    "#     # duplicate hashes have been found\n",
    "#     for hashval, array_of_ids in dict_of_duplicate_docs.items():\n",
    "#       if len(array_of_ids) > 1:\n",
    "#         print(\"********** Duplicate docs hash=%s **********\" % hashval)\n",
    "#         # Get the documents that have mapped to the current hashval\n",
    "#         matching_docs = es.mget(index='istp', doc_type=\"doc\", body={\"ids\": array_of_ids})\n",
    "#         #for doc in matching_docs['docs']:\n",
    "#         for num, doc in enumerate(matching_docs['docs'], start=2):\n",
    "#             # In this example, we just print the duplicate docs.\n",
    "#             # This code could be easily modified to delete duplicates\n",
    "#             # here instead of printing them\n",
    "#             print(\"doc=%s\\n\" % doc)\n",
    "#             #DELETE /<index>/_doc/<_id>\n",
    "#             if num > 2:\n",
    "#                 es.delete(index='istp', id=doc[\"_id\"])\n",
    "# def main():\n",
    "#     #for index in es.indices.get('*'):\n",
    "#         #if(re.match(r'^[e|i]', index)):\n",
    "#             scroll_over_all_docs()\n",
    "#             loop_over_hashes_and_remove_duplicates()\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import asyncio\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "#https://www.elastic.co/blog/how-to-find-and-remove-duplicate-documents-in-elasticsearch\n",
    "es = Elasticsearch([\"localhost:9200\"])\n",
    "dict_of_duplicate_docs = {}\n",
    "# The following line defines the fields that will be\n",
    "# used to determine if a document is a duplicate\n",
    "keys_to_include_in_hash = [\"url\", \"title\"]\n",
    "# Process documents returned by the current search/scroll\n",
    "async def populate_dict_of_duplicate_docs(hits):\n",
    "    \n",
    "    for item in hits:\n",
    "        combined_key = \"\"\n",
    "        for mykey in keys_to_include_in_hash:\n",
    "            combined_key += str(item['_source'][mykey])\n",
    "        _id = item[\"_id\"]\n",
    "        hashval = hashlib.md5(combined_key.encode('utf-8')).digest()\n",
    "        # If the hashval is new, then we will create a new key\n",
    "        # in the dict_of_duplicate_docs, which will be\n",
    "        # assigned a value of an empty array.\n",
    "        # We then immediately push the _id onto the array.\n",
    "        # If hashval already exists, then\n",
    "        # we will just push the new _id onto the existing array\n",
    "        dict_of_duplicate_docs.setdefault(hashval, []).append(_id)\n",
    "        return dict_of_duplicate_docs\n",
    "# Loop over all documents in the index, and populate the\n",
    "# dict_of_duplicate_docs data structure.\n",
    "async def scroll_over_all_docs(index):\n",
    "    dict_of_duplicate_docs = {}\n",
    "    data = es.search(index=index, scroll='1m',  body={\"query\": {\"match_all\": {}}})\n",
    "    # Get the scroll ID\n",
    "    sid = data['_scroll_id']\n",
    "    scroll_size = len(data['hits']['hits'])\n",
    "    # Before scroll, process current batch of hits\n",
    "    populate_dict_of_duplicate_docs(data['hits']['hits'])\n",
    "    while scroll_size > 0:\n",
    "        data = es.scroll(scroll_id=sid, scroll='2m')\n",
    "        # Process current batch of hits\n",
    "        populate_dict_of_duplicate_docs(data['hits']['hits'])\n",
    "        # Update the scroll ID\n",
    "        sid = data['_scroll_id']\n",
    "        # Get the number of results that returned in the last scroll\n",
    "        scroll_size = len(data['hits']['hits'])\n",
    "        \n",
    "    return dict_of_duplicate_docs\n",
    "    \n",
    "        \n",
    "    #await loop_over_hashes_and_remove_duplicates(index)\n",
    "    \n",
    "def loop_over_hashes_and_remove_duplicates(index, dict_of_duplicate_docs):\n",
    "    # Search through the hash of doc values to see if any\n",
    "    # duplicate hashes have been found\n",
    "    for hashval, array_of_ids in dict_of_duplicate_docs.items():\n",
    "      if len(array_of_ids) > 1:\n",
    "        print(\"********** Duplicate docs hash=%s **********\" % hashval)\n",
    "        # Get the documents that have mapped to the current hashval\n",
    "        matching_docs = es.mget(index=index, doc_type=\"doc\", body={\"ids\": array_of_ids})\n",
    "        #for doc in matching_docs['docs']:\n",
    "        for num, doc in enumerate(matching_docs['docs'], start=2):\n",
    "            # In this example, we just print the duplicate docs.\n",
    "            # This code could be easily modified to delete duplicates\n",
    "            # here instead of printing them\n",
    "            print(\"doc=%s\\n\" % doc)\n",
    "            #DELETE /<index>/_doc/<_id>\n",
    "            if num > 2:\n",
    "                es.delete(index=index, id=doc[\"_id\"])\n",
    "#def main():\n",
    "    #for index in es.indices.get('*'):\n",
    "        #if(re.match(r'^[e|i]', index)):\n",
    "            #scroll_over_all_docs()\n",
    "            #loop_over_hashes_and_remove_duplicates()\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def deleteIndex(index):\n",
    "    try:\n",
    "        dict = await scroll_over_all_docs(index)\n",
    "        loop_over_hashes_and_remove_duplicates(index, dict)\n",
    "        print('success')\n",
    "    except e as Exception:\n",
    "        print(e)\n",
    "    #await loop_over_hashes_and_remove_duplicates(index, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "enfj\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f7161cff597b>\u001b[0m in \u001b[0;36mdeleteIndex\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mloop_over_hashes_and_remove_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'success' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9f5ce85f06d3>\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-f7161cff597b>\u001b[0m in \u001b[0;36mdeleteIndex\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mloop_over_hashes_and_remove_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#await loop_over_hashes_and_remove_duplicates(index, dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for index in es.indices.get('*'):\n",
    "    if(re.match(r'^[e|i]', index)):\n",
    "        print(index)\n",
    "        await deleteIndex(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-e0574549007e>, line 4)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-e0574549007e>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    if(re.match(r'^[e|i]', index)):'comment', scroll='1m',  body={\"query\": {\"match_all\": {}}})\u001b[0m\n\u001b[1;37m                                                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for index in es.indices.get('*'):\n",
    "    if(re.match(r'^[e|i]', index)):'comment', scroll='1m',  body={\"query\": {\"match_all\": {}}})\n",
    "        # Get the scroll ID\n",
    "            sid = data['_scroll_id']\n",
    "                scroll_size = len(data['hits']'comment'\n",
    "        await deleteIndex(index)\n",
    "        #tasks = scroll_over_all_docs(index)\n",
    "        #await asyncio.wait(tasks)\n",
    "        #loop_over_hashes_and_remove_duplicates(index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}